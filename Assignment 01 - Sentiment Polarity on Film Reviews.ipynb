{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4ly85dOjwrP"
   },
   "source": [
    "# Assignment 1: Sentiment Polarity for Film Reviews\n",
    "\n",
    "*version 2025.1, details/bugs in this assignment might be patched, you will be notified if this happens*\n",
    "\n",
    "For this assignment, you will implement a classification pipeline for a binary sentiment classification task. The provided dataset includes film reviews, labelled as either positive or negative. You are given an implementation of the full pipeline that uses scikit learn and example data as a starting point. Your task is to reimplement this pipeline.\n",
    "\n",
    "In most real-world situations, you will have good (and relatively bug free) tools at your disposal. However, for being able to be creative with an ML problem, it is crucial to understand the inner workings of a full pipeline. You will therefore re-implement the model, feature extraction, the learning algorithm, and the prediction algorithm. You will find re-implementation tasks marked \"implementation task\" (in bold) below. In your submission, all code must be written from scratch by you. While you can use the base modules of python (os, string etc), and NumPy and SciPy for array/matrix operations, you cannot use sklearn or any equivalent ML library (when in doubt, ask about imports). It is recommended to use the reference pipeline while implementing each part of your own functionality. This way, you can try your own code throughout, without having to get all the moving parts in place before testing.\n",
    "\n",
    "This assigment is expected to take time, so give yourself space to digest it. Think about what is hard and what you can start doing. When you know what to do, the coding takes less time that most expect. So take advantage of that this is a campus course where there are people being payed to help you. Also, you are not expected to understand all the maths, only enough to implement it. Separating the two is a crucial part of the assignment.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Please submit your code as a notebook through studium. You should include the following:\n",
    "\n",
    "1. A working implementation of your pipeline, reproducing your principal results when run. Please rerun your notebook as the last thing you do before submitting.\n",
    "2. Comment the code properly, especially for longer or opaque functionality. Please try to write self documenting code (i.e., choosing descriptive variables names, refactoring to isolate functionality, minimal code duplication etc).\n",
    "3. A *brief* description (100-200 words in total) of the implementation work that was necessary to complete the different parts, showing how you arrived at your solution and design choices made. You can spread these in the notebook or put them in one place.\n",
    "4. Comments on what you thought was hard in the assignment, what you think was educational, what took most time and which parts might be unnecessarily tricky.\n",
    "5. As submissions are anonymous, **all personal information must be removed**. This is a hard requirement.\n",
    "6. Apart from section titles, please remove all unnecessary text and code from the notebook you hand in. Keep only that which strengthens the case that you fulfil the listed requirements. All notebooks containing unnecessary chunks of text from this instruction will receive a failing grade (U).\n",
    "\n",
    "## Requirements for grade G\n",
    "\n",
    "To achieve a pass (G) in this assignment, you must solve the following tasks without serious errors.\n",
    "\n",
    "1. Reimplement the four parts of the assignment. The instructions below will guide you on specifics for each part.\n",
    "2. When evaluating your model, split the data into a training and a test set. This split can be selected non-randomly before running any training. The split should be 80/20 (i.e., 1600 training documents and 400 test documents). *It is easier for you if the classes are balanced.*\n",
    "3. Your code should work as plug-in replacements for the vectorizer and model. Hence, you should implement the model and the vectorizer as classes following the sklearn API as:\n",
    "\n",
    "```\n",
    "class feature_transform:\n",
    "  def __init__(self, ...):\n",
    "    ...\n",
    "\n",
    "  def fit_transform(self, X):\n",
    "    ...\n",
    "\n",
    "\n",
    "class model:\n",
    "  def __init__(self, learning_rate, ...):\n",
    "    ...\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    ...\n",
    "\n",
    "  def predict(self, X):\n",
    "    ...\n",
    "\n",
    "  def score(self, X, y):\n",
    "    ...\n",
    "```\n",
    "4. Include a short (150 words) analysis in your submission. Discuss the final decision boundary (i.e., why did some word contribute to a negative/positive label), the selected hyperparameters, and design choices. Note that hyperparameters do not need to be systematically selected, use something that works.\n",
    "\n",
    "### Additional requirements for grade VG\n",
    "\n",
    "To achieve a pass with distinction (VG) in this assignment, you must adequately solve the tasks above for a passing grade (G). In addition, you must:\n",
    "\n",
    "1. Implement the optimization as some version of *stochastic* gradient descent (SGD).\n",
    "2. Implement a [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature model, and compare classification performance to bag-of-words (this should also be briefly discussed in your analysis). Choose your preferred formulation of tf-idf from the literature, *briefly* motivating your choice.\n",
    "3. Implement an extension to the SDG optimization of your choice, e.g. from [this list on wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants).\n",
    "4. Implement [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) for evaluating and comparing your model variants.\n",
    "5. Prepare a presentation (~5min) with analysis of your design choices, pipelines, and results. How much did you gain in performance by using more complex methods in your pipelines? The analysis and claims must be essentially correct. Submit and handful of slides in pdf format with your notebook.\n",
    "\n",
    "## General advice\n",
    "\n",
    "The task is to predict, for an unseen review, whether it is positive or negative. This is a binary classification task. Work from the given code and change one piece at a time. You should test your code every couple of lines to make sure your assumptions on functionality and variable content are correct. A good rule of thumb is that a coder will introduce a bug every five lines (even as a professional).\n",
    "\n",
    "To make sure your code does what it is supposed to do, use ```assert``` statements to check your assumptions. Keep the given asserts if you need them. Professional coders sometimes start with writing tests for some functionality instead of starting with the functionality itself. This is called *test-driven development*.\n",
    "\n",
    "When developing, it is often best to start with *simple and clear code*, then going on to think about efficiency. Writing efficient code will not help you if you've missunderstood parts of the task description. For example, when finding the gradient, start by creating the vector and running a `for` loop over the data. Only then do you go on to experiment with vector operations and sparse matrices (assuming you want to).\n",
    "\n",
    "## Plagiarism\n",
    "\n",
    "In code assignments, plagiarism is a tricky concept. A clean cut way would be to demand that you write all the code yourself, from memory, with only the assigned literature as help. This is not how code is developed professionally, where it is common to copy and share. However, since this is a learning exercise, you must implement everything on your own, but please look at the course repo, Stack Overflow etc. Moreover, discuss with course mates and TAs to find inspiration and solutions. Code that is *obviously* copied (with minor modifications) will be considered as plagiarized. As a rule of thumb, if you can explain a solution to some problem in a brief discussion, you're safe. If you share your code, even with the best of intentions (or due to social pressure), the submitted (or committed) code with the earliest time stamp will be considered as the original. As a part of the examination, you might be asked to explain any particular part of the functionality in your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY-34D1MwuBd"
   },
   "source": [
    "## Part 1: Parsing the dataset\n",
    "\n",
    "For this assignment, we use the [Review polarity v2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/) data set created by Bo Pang and Lillian Lee at Cornell University. It consists of 2000 movie reviews, 1000 of which are positive and 1000 are negative. *Always check the readme for any dataset before using it.*\n",
    "\n",
    "The following downloads the dataset (if it's not already present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gd9vXkT0Uf4_"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"\"\"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\"\"\n",
    "if not os.path.exists(url.split(\"/\")[-1]):\n",
    "  urlretrieve(url, url.split(\"/\")[-1])\n",
    "  print(\"Downloaded\", url)\n",
    "  with tarfile.open('review_polarity.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "  print(\"Extracted archive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_NqWr2p555X"
   },
   "source": [
    "In order to get the given code to work, a part of the 20 newsgroup dataset is loaded below. This should be replaced by your parsing code in your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73pgNoj_N4cZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "X_raw, y = fetch_20newsgroups(subset='train', categories=['rec.sport.baseball', 'rec.sport.hockey'], remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
    "y[y==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZGYeoyxhh-u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assert len(X_text) == 2000\n",
    "assert np.all([isinstance(x, str) for x in X_raw])\n",
    "assert len(X_raw) == y.shape[0]\n",
    "assert len(np.unique(y))==2\n",
    "assert y.min() == -1\n",
    "assert y.max() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6LSWjDIwIL0"
   },
   "source": [
    "**Implementation task:** Implement a parser for the dataset. The output should be a list/array of strings (`X_raw`) and a list/array of labels (`y`) encoded as {-1,1}.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K-Lo5iM-Nnr"
   },
   "outputs": [],
   "source": [
    "# Splitting off the test set\n",
    "split_point = int(len(X_raw)*0.8)\n",
    "X_train = X_raw[:split_point]\n",
    "X_test = X_raw[split_point:]\n",
    "y_train = y[:split_point]\n",
    "y_test = y[split_point:]\n",
    "\n",
    "assert len(y_train)+len(y_test) == len(y)\n",
    "assert len(y_test) > 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_F1ortjjs_u"
   },
   "source": [
    "## Part 2: Feature extraction\n",
    "\n",
    "As basic features, we use a binary bag-of-words (BOW) representation of the words in each review. Each review in the data set is described by a vector with one element corresponding to each word in the vocabulary. An element is set to 1 if the review contains its associated word, otherwise it is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOUPbUj_OP-W"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)             # Creates the vocabulary of the vectorizer\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_train = X_train.todense()                   # sklearn will output a sparse matrix\n",
    "X_train[X_train>1] = 1                        # Turns the count vectors into binary vectors\n",
    "X_train = np.asarray(X_train)                 # Turns the matrix into an array. SGD model doesn't support matrix in the newer sklearn module version\n",
    "\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_test = X_test.todense()                   # sklearn will output a sparse matrix\n",
    "X_test[X_test>1] = 1                        # Turns the count vectors into binary vectors\n",
    "X_test = np.asarray(X_test)                 # Turns the matrix into an array. SGD model doesn't support matrix in the newer sklearn module version\n",
    "\n",
    "ordered_vocabulary = vectorizer.get_feature_names_out()\n",
    "vocabulary = set(ordered_vocabulary)\n",
    "\n",
    "assert X_train.shape[1] == X_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xBHQgd9YjSW"
   },
   "source": [
    "If needed, we can do lookup tables going from tokens to feature numbers. Note that most of the elements in any feature vector will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziRzSdAgYJiB"
   },
   "outputs": [],
   "source": [
    "lut = dict()\n",
    "for i, word in enumerate(ordered_vocabulary):\n",
    "  lut[word] = i\n",
    "\n",
    "for word in ['dolphin', 'the', 'coffee']:\n",
    "  if word in vocabulary:\n",
    "    print(\"'%s' is represented as feature dimension %i\" %(word, lut[word]))\n",
    "  else:\n",
    "    print(\"'%s' is not in the vocabulary\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJIW3aaM3_Yf"
   },
   "source": [
    "**Implementation task:** You should re-implement the feature extraction above. The list/array called `ordered_vocabulary` should contain the words for each feature dimension, and X should contain the BOW binary vectors. Remember to use the same method names as the original sklearn class.\n",
    "\n",
    "*Hints: Implementing X as a NumPy array or a SciPy sparse matrix (not as a list) will make your life easier in the coming parts.*\n",
    "\n",
    "We can now look at the data and the words corresponding to feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aVgDJ0mksJ2"
   },
   "outputs": [],
   "source": [
    "print(ordered_vocabulary[2000:2010])\n",
    "print(X_train[:10, 2000:2010])\n",
    "for w in ['dolphin', 'the', 'coffee']:\n",
    "  print(\"'%s' in vocabulary: %s\" % (w, w in vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y9pWyL1ju7P"
   },
   "source": [
    "At this point in the code, X and y are ready for the supervised learning task.\n",
    "\n",
    "## Part 3: Learning framework\n",
    "\n",
    "The model we will use is a simple hyperplane. This plane will represent the decision boundary through the data space, separating positive from negative ratings. Note that the following three parts are independent, and can be implemented in separate methods.\n",
    "\n",
    "**Implementation task:** You should implement your versions of the following parts (you can also find this in the slides):\n",
    "\n",
    "1. **Hyperplane model**. The model should be a hyperplane as $f(x, \\omega) = sgn(\\omega^\\top x)$, where $sgn(\\cdot)$ is the [sign function](https://en.wikipedia.org/wiki/Sign_function). Note that $x_0$ in this notation is the pseudo input 1. When evaluated, this gives us the predicted results as $\\hat y_i = f(X_i, \\omega^{(t)})$, where $\\omega^{(t)}$ is the parameter vector at optimization iteration t and $\\omega^{(0)}$ is the initial guess for the parameter vector.\n",
    "\n",
    "2. **Objective function.** The loss function for our model is the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss), which will be used together with $l_2$ regularization.\n",
    "\n",
    "  $\\mathfrak{L}(X, y, \\omega) = \\frac{\\lambda}{2} ||\\omega||^2 + \\sum_{i=1}^{|X|} \\max(0, 1-y_i \\cdot \\omega^\\top X_i)$.\n",
    "\n",
    "  Regularization is done by adding a norm on the parameter vector and including that in the objective function. A shorter parameter vector gives a larger margin for this model. The $l_2$ norm is defined as $\\sqrt{\\sum_{i=1}^n \\omega_i^2}$. The regularization always has some positive attenuation parameter $\\lambda \\in \\mathbb{R}$ keeping it from dominating the objective function. It symbolizes a trade-off between a more accurate classification and wider margins, while also giving the objective function a unique solution.\n",
    "\n",
    "3. **Gradient descent**. The update for gradient descent looks like $\\omega^{(t)} = \\omega^{(t-1)} - \\gamma \\nabla \\mathfrak{L}(\\omega^{(t-1)})$, where the update gradient is defined as $\\nabla \\mathfrak{L}(X, y, \\omega) = \\left ( \\frac{\\partial \\mathfrak{L}(X, y, \\omega)}{\\partial \\omega_0}, \\frac{\\partial \\mathfrak{L}(X, y, \\omega)}{\\partial \\omega_1}, \\ldots, \\frac{\\partial \\mathfrak{L}(X, y, \\omega)}{\\partial \\omega_n} \\right )^\\top$. The expression for this gradient $\\nabla \\mathfrak{L}$ is given analytically as:\n",
    "\n",
    "  $\\nabla \\mathfrak{L}(X, y, \\omega) = \\lambda \\omega + \\sum_{i=1}^{|X|}\n",
    "\\begin{cases}\n",
    "0 & \\text{if } y_i \\omega^\\top X_i \\geq 1\\\\\n",
    "-y_i X_i & \\text{else}\n",
    "\\end{cases}$\n",
    "\n",
    "In the expression for the gradient, $X_i$ is a vector (with pseudo input) and $y_i$ is a scalar. These two refer to the i:th data point and its label. The learning rate $\\gamma \\in \\mathbb{R}$ acts as a scaling/dampening factor on the gradient update. This should run until some stopping criteria is met (e.g., $\\omega^{(t+1)}\\approx \\omega^{(t)}$). In `SGDClassifier` from sklearn, this is interpreted as finding $loss_{current} > loss_{best} - .001$ for five consecutive iterations.\n",
    "\n",
    "*Note that while your code will be runnable, it will likely be much slower than sklearn's code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkQXKQx-yTBW"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Set hyperparameters (these variables are only here for clarity)\n",
    "reguliser_dampening = 0.001   # lambda\n",
    "learning_rate = .1            # gamma\n",
    "\n",
    "# Create the untrained classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=reguliser_dampening, verbose=1,\n",
    "                      learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "# Train the classifier\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the parameter vector\n",
    "omega = np.concatenate([model.intercept_, model.coef_.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vI1mGxUpljl5"
   },
   "source": [
    "In the above training with verbose=1, note how the loss etc are changing. For your implementation, it can be very good to print out lots of information so that you can see if you get what you expect (e.g. a lowering of the loss).\n",
    "\n",
    "We can examine the weights by plotting them. Think about how to interpret these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdyxUJFdO0wS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(omega[1:])\n",
    "plt.xlabel(\"Value\")\n",
    "plt.xlabel(\"Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12tJY8d8ma3V"
   },
   "source": [
    "From the same information, we can plot the words with the strongest influence. Can you see a pattern? Is the model over learning on some words? Why might word frequency be important when analysing their impact?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM3Nm2_2l2j3"
   },
   "outputs": [],
   "source": [
    "assert (len(omega)-1) == len(vocabulary)\n",
    "\n",
    "# Sort by absolute value\n",
    "idx = np.argsort(np.abs(omega[1:]))\n",
    "\n",
    "print(\"                Word   Weight  Occurences\")\n",
    "for i in idx[-20:]:   # Pick those with highest 'voting' values\n",
    "  print(\"%20s   %.3f\\t%i \" % (ordered_vocabulary[i], omega[i+1], np.sum([ordered_vocabulary[i] in d for d in X_raw])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSlwPrX1wVCq"
   },
   "source": [
    "## Part 4: Exploring hyperparameters\n",
    "\n",
    "For optimization of the hyperparameters, you can search for values on a grid. Trying all combinations is called a grid search and can be implemented with nested `for` loops. A faster alternative is to sample from the grid. Sampling is not as thorough, but most often sufficient (and much faster). Following the current consensus on ML methodology, we must split off a test set before exploring any configuration and use this data only at the very end of the experiment.\n",
    "\n",
    "**Implementation task:** Implement code for printing a sorted table of your sampled hyperparameters. Note, you do not have to reimplement the grid search.\n",
    "\n",
    "1. **Learning rate**. Whe  trying out different learning rates, it is useful to pick values from an exponentially spaced grid (e.g., 0.0001/0.0003/0.001/0.003/0.01/0.03/0.1/0.3/1.0/3.0). Note what happens when the learning rate gets too small or too large. The best learning rate is as large as possible, making the optimization converge faster, but still reliable and stable.\n",
    "\n",
    "2. **Regulariser dampening**. Try to find how much the regulariser needs to be dampened to get a good score. Use an exponentially spaced grid here as well.\n",
    "\n",
    "The grid points can be generated as `np.logspace(-4, 3, num=1000))`, and pick a random subset by running `np.random.choice(your_array, size=10)` (the code below generates 1000000 grid points but only tries 10 of them). For these real valued hyperparameters, you can add extra points if you find a promising interval. Note that trying every combination of hyperparameters is likely not feasible, but don't hesitate to try.\n",
    "\n",
    "Once you’ve settled on a final set of hyperparameters that work well/decently (and only then!), use the test set to obtain an accuracy score on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "n_samples = 10\n",
    "best_hyperparameters = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stah3yXmM44-"
   },
   "outputs": [],
   "source": [
    "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
    "for learning_rate, reguliser_dampening in zip(np.random.choice(np.logspace(-4, 3, num=1000), n_samples),\n",
    "                                              np.random.choice(np.logspace(-4, 3, num=1000), n_samples)):\n",
    "\n",
    "  # Set up the classifier\n",
    "  model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                        alpha=reguliser_dampening, verbose=0,\n",
    "                        learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "  # Train the classifier\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Calculate the training accuracy\n",
    "  training_accuracy = np.sum(model.predict(X_train)==y_train)/len(y_train)\n",
    "\n",
    "  # Store the hyperparameters if they are better than what we have found before\n",
    "  if best_hyperparameters is None or best_hyperparameters[2] < training_accuracy:\n",
    "    best_hyperparameters = (learning_rate, reguliser_dampening, training_accuracy)\n",
    "  print(\"%.3f\\t\\t%.3f\\t\\t%.1f%%\" % (learning_rate, reguliser_dampening, 100*training_accuracy))\n",
    "\n",
    "best_learning_rate = best_hyperparameters[0]\n",
    "best_reguliser_dampening = best_hyperparameters[1]\n",
    "print(\"Best parameters: %.5f, %.5f\" % (best_learning_rate, best_reguliser_dampening))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsxPiVN2NG8M"
   },
   "source": [
    "We can now run the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-4dV6Mp8vxS"
   },
   "outputs": [],
   "source": [
    "# Set up the classifier\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=best_reguliser_dampening, verbose=0,\n",
    "                      learning_rate='constant', eta0=best_learning_rate)\n",
    "\n",
    "# Train on all the non-test data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Run prediction on the test set\n",
    "test_accuracy = np.sum(model.predict(X_test)==y_test)/len(y_test)\n",
    "\n",
    "print(\"Test set accuracy %.1f%%\" % (100*test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3vhjy-DwpKJ"
   },
   "source": [
    "Is this a good result? What is a good result?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
